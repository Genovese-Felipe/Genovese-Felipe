Guia para o Code Human Preference Eval: Revisão e Plano de Execução
Análise do Guia Atual
O guia elaborado pelo usuário mostra-se bastante completo e alinhado com as exigências do teste Code Human Preference Eval. A seguir, verificamos cada ponto crítico exigido e se foi devidamente coberto:
• Qualidade e Estrutura dos Prompts: O guia enfatiza fortemente a criação de prompts de alta qualidade e bem estruturados. Ele menciona um “Padrão de Ouro” para prompts de engenharia, com seções claras (feature, contexto/porquê, requisitos de UI/UX, sugestões técnicas e critérios de aceitação). Essa estrutura garante prompts bem escopados (nem ambíguos nem muito amplos) e focados no “o quê” e “por quê”, evitando prescrever “como” implementar. Essa abordagem atende ao critério de que a qualidade do prompt é fundamental para o sucesso, conforme destacado nas instruções oficiais. Não há lacunas aqui – o guia inclusive fornece exemplos concretos de prompts bons vs. ruins e recomenda seguir o modelo profissional (como se fosse uma especificação de Jira/Linear).
• Clareza na Justificativa das Avaliações: O guia deixa claro que apenas gerar código não basta; é crucial justificar e avaliar cada resposta do modelo com profundidade técnica. Há orientação específica para escrever justificativas em nível sênior, indo além do genérico. Por exemplo, ele sugere mencionar princípios de engenharia (SOLID, DRY), discutir manutenibilidade, testabilidade e robustez, e comparar abordagens dos modelos (ex.: Modelo A vs Modelo B) com evidências concretas do processo de cada um. Essa ênfase atende à exigência do teste de que a clareza e qualidade da sua rationale influenciam diretamente a avaliação. O guia chega a fornecer uma estrutura de resposta (tese, evidências do processo, prós e contras, conclusão) e exemplos de frases para fortalecer a argumentação – ou seja, está cobrindo muito bem a necessidade de justificativas claras e convincentes.
• Cumprimento de 2 a 3 Turnos Independentes: O material deixa explícito que a avaliação consiste em 2 a 3 ciclos independentes, cada um sendo uma nova conversa (um prompt e uma resposta) sem dependência nos anteriores. O guia reforça a regra de não submeter antes de completar pelo menos 3 turnos (exigência da plataforma). Também orienta a planejar bem cada turno, garantindo que cada tarefa comece e termine dentro daquele único prompt-resposta. Não há lacuna aqui – o guia compreende corretamente que são necessárias no mínimo 3 interações separadas, cada qual bem definida, conforme as regras.
• Boa Exploração do Repositório AI ChatKit: O guia recomenda clonar e estudar o repositório pasonk/ai-chatkit minuciosamente antes de elaborar os prompts. Ele orienta identificar a estrutura, tecnologias usadas (React, Next.js, TypeScript, etc.) e entender como os componentes interagem. Em seguida, sugere derivar tarefas realistas a partir dessa análise – por exemplo, pequenos recursos, refatorações ou testes que “se encaixem naturalmente no projeto”. Isso demonstra boa exploração do codebase: em vez de pedir algo irrelevante ou impossível, o usuário irá propor melhorias/problemas genuínos do ChatKit. Essa preparação está de acordo com as dicas do teste (como considerar rodar o código gerado e garantir que a solução é verificável). Se há algo a ajustar aqui, talvez seja apenas reforçar que explorar o repositório inclui entender problemas existentes ou limitações atuais para inspirar tarefas – mas de forma geral, o guia cobre bem a necessidade de embasamento no repositório real.
• Demonstração de Pensamento de Engenharia Sênior: Este é um dos pontos mais fortes do guia. Ele consistentemente incentiva o usuário a agir e pensar como um engenheiro de software sênior ao usar a LLM:
• Recomenda verificar suposições do modelo (ex.: se o Modelo A “chutou” nomes de funções ou arquivos, conferir no repo se estão corretos).
• Valoriza abordagens robustas (como a do Modelo B, que investiga antes de codar) e encoraja elogiar estratégias de TDD, separação de preocupações, cobertura de testes, acessibilidade etc.
• Orienta criticar soluções arriscadas ou descuidadas, e enaltecer soluções elegantes e manuteníveis.
• Sugere usar vocabulário técnico apropriado e mencionar princípios de design de software nas justificativas.
Tudo isso indica fortemente uma mentalidade de engenharia sênior sendo promovida. O guia poderia ainda sugerir rodar/localmente o código produzido ou pensar em casos de teste adicionais, mas na essência ele já foca em qualidade de código de produção e boas práticas. Portanto, este critério está bem atendido – o usuário, seguindo o guia, demonstrará rigor e maturidade na análise.
Em resumo, o guia atual abrange todos os pontos críticos exigidos pelo teste. Trata-se de um material detalhado e bem alinhado com as instruções da Alignerr. As melhorias a fazer são sutis, como veremos a seguir, pois o essencial está coberto.
Ajustes Recomendados
Embora o guia esteja completo, alguns ajustes finos podem torná-lo ainda mais infalível:
• Diversidade de Tarefas: Certifique-se de planejar tarefas distintas entre si para os 2–3 turnos, demonstrando versatilidade. Por exemplo, combinar diferentes tipos de demandas – um turno focado em novo feature de front-end, outro em refatoração ou melhoria de performance, outro talvez cobrindo testes unitários ou tratamento de erro. A própria Alignerr sugere mostrar breadth of knowledge (amplitude de conhecimento) através da diversidade de prompts. O guia já traz exemplos variados; apenas garanta que suas escolhas de tarefas não fiquem todas na mesma categoria.
• Validação do Código Gerado: Se o tempo permitir durante a prova (1h30 no total), tente verificar na prática partes do código que o modelo fornecer. Por exemplo, após o modelo responder, você pode compilar o projeto localmente, rodar os testes ou ao menos revisar o patch para ver se não quebra nada. O guia menciona analisar o codebase e confiar na inspeção visual, o que é ótimo; mas testar executando o código (quando possível) é um diferencial. Isso daria ainda mais subsídios para sua justificativa (você poderia mencionar se o código rodou corretamente, se passou em testes, etc.). Contudo, gerencie bem o tempo – só execute códigos pequenos ou testes rápidos para não comprometer outras partes.
• Observância do Tom e Autoria: Lembre-se da regra de que o uso de IA é permitido porém não pode ser detectável. Ou seja, a forma final das respostas e justificativas deve parecer escrita por um humano. O guia já orienta escrever de forma profissional e técnica (o que é correto), mas fique atento para remover qualquer traço de resposta genérica ou “voz de AI”. Personalize a escrita conforme seu estilo, mantenha-a coerente e natural. Isso vale especialmente ao usar o ChatGPT como apoio: use-o para ideias e revisão, mas redija as respostas finais com seu toque pessoal.
• Clareza nas Regras de Envio: Tenha em mente (e o guia poderia destacar explicitamente) a regra dos 3 turnos completos antes de submeter. Você só conclui a tarefa após finalizar todas as interações requeridas. Cada turno deve ser bem aproveitado – não corra para submeter cedo. Use todos os turnos para maximizar sua pontuação, já que você só tem uma tentativa. Esse ponto já está implícito no guia, mas vale reforçar: mantenha a calma e complete todas as etapas conforme as regras.
• Tempo e Escopo Realista: O guia foca bastante na qualidade, o que é ótimo, mas lembre de calibrar o escopo das tarefas ao tempo disponível. Tarefas bem escopadas são por definição possíveis de concluir em uma resposta; evite prompts que poderiam gerar soluções gigantescas ou demandar muitas alterações no projeto inteiro. Siga as dicas do guia (escopo limitado, problema específico e verificável) para que o modelo consiga resolver em ~15 minutos de geração. Planeje também o tempo para escrever suas justificativas com calma – a banca avaliará cada palavra delas.
Implementando esses pequenos ajustes, o guia ficará ainda mais sólido. Agora, apresentamos um plano passo a passo de execução da avaliação, mostrando como o usuário pode seguir esse guia com a assistência direta do ChatGPT em cada etapa.
Plano de Execução com Assistência do ChatGPT
A seguir está um plano estruturado para o usuário realizar o Code Human Preference Eval do Alignerr, desde a preparação até a submissão. Em cada etapa destacamos como o ChatGPT pode ajudar ativamente. Lembre-se de que o ChatGPT aqui é sua ferramenta de apoio nos bastidores – use-o para se preparar e revisar, mas a resposta final na plataforma deve ser sua redação original.
Passo 1: Leitura e Preparação do Repositório (AI ChatKit)
Antes de tudo, faça o fork/clone do repositório AI ChatKit localmente e explore seu conteúdo. Navegue pela estrutura de pastas, principais componentes e leia o README se houver. Identifique as tecnologias (React, TS, etc.) e funcionalidades existentes. Como o ChatGPT pode ajudar: você pode pedir ao ChatGPT um resumo da arquitetura do projeto ou explicações de partes do código. Por exemplo: “Estou olhando o arquivo chat.tsx e use-chat-store.ts. Pode me explicar brevemente o papel desses componentes no AI ChatKit?”. O ChatGPT pode fornecer uma visão geral que agilize seu entendimento. Use-o também para esclarecer trechos de código confusos ou conceitos do framework (ex.: “O que essa implementação usando Zustand está fazendo?”). Essa preparação garante que você tenha contexto suficiente para pensar em tarefas relevantes.
Passo 2: Brainstorm de Tarefas Bem Escopadas
Com uma visão do projeto, liste potenciais melhorias ou features pequenas que façam sentido dentro do AI ChatKit. Pense como um desenvolvedor identificando to-dos: algo que melhore a UX, corrija uma limitação ou adicione uma funcionalidade incremental. Como o ChatGPT pode ajudar: use-o como sparring para gerar ideias de tarefas. Você pode descrever brevemente o projeto e pedir sugestões: “Que melhorias incrementais poderiam ser implementadas nesse chat app em React? Preciso de algo pequeno e testável.” O ChatGPT pode sugerir coisas como “adicionar botão de copiar código”, “funcionalidade de editar última mensagem”, “melhorar tratamento de erros de rede”, etc. (Inclusive, essas ideias apareceram nos exemplos analisados, então você já sabe que são boas candidatas). Anote 3 ou 4 ideias e avalie com o ChatGPT se elas são verificáveis e não ambíguas. Por exemplo, confirme: “Se eu pedir X, dá para saber claramente quando está pronto?” ou “Y seria muito grande para um turno único?”. Refine as ideias até ter 2 ou 3 tarefas bem definidas para usar nos turnos.
Passo 3: Seleção das 2–3 Tarefas e Definição do Escopo
Escolha as tarefas finalistas equilibrando viabilidade e diversidade. Por exemplo, suponha que você selecione: (1) Adicionar funcionalidade “Copy Code” nos blocos de código, (2) Refatorar componente de chat para separar responsabilidades, (3) Implementar deleção de histórico de conversa. Garanta que cada uma seja independente das outras (não reaproveite código ou contexto entre turnos). Como o ChatGPT pode ajudar: discuta cada ideia selecionada para garantir que o escopo está correto. Você pode perguntar: “A tarefa X está suficientemente específica? Como posso torná-la mais clara?”. O assistente irá lembrar das regras (por exemplo, evitar subjetividade, definir critérios de aceitação) e pode sugerir ajustes. Nesta fase, defina também o resultado esperado de cada tarefa (ex.: “botão de copiar funcionando em todos code blocks, com feedback visual e testes unitários”). Assim, você já tem em mente o que esperar do modelo.
Passo 4: Elaboração dos Prompts de Alta Qualidade
Agora, transforme cada ideia de tarefa em um prompt exemplar, seguindo a estrutura profissional de 5 partes identificada no guia:
• Descrição breve da Feature/Requisito – ex: “Implementar funcionalidade de copiar conteúdo de blocos de código no chat (Copy Code)”.
• Contexto e Porquê – ex: “Usuários frequentemente precisam copiar trechos de resposta; adicionar essa facilidade melhora a UX sem afetar o backend.”.
• Detalhes de UI/UX e Escopo – ex: “Ao passar o mouse em um bloco de código, mostrar um botão ‘Copy’. Ao clicar, copiar texto para clipboard, mostrar toast ‘Copied’ por 1.5s. Suportar acessibilidade (aria-live announcement).”.
• Sugestões Técnicas (sem ditar solução) – ex: “Pode usar navigator.clipboard.writeText. Talvez criar utilitário copyToClipboard.ts para centralizar essa lógica de clipboard e erros.”.
• Critérios de Aceitação/Teste – ex: “Funcional em blocos longos, código copiado exatamente igual. Incluir teste unitário do utilitário e teste de componente para o botão.”.
Escreva um rascunho do prompt para cada tarefa seguindo esse modelo. Como o ChatGPT pode ajudar: peça para revisar e melhorar seus prompts. Por exemplo: “Este é meu prompt para a funcionalidade X: [texto]. Está claro e bem estruturado?”. O ChatGPT pode apontar se algo ficou ambiguo, subjetivo ou faltando detalhe. Ele pode sugerir frases mais objetivas ou lembrar de incluir algum requisito esquecido. Itere com o assistente até os prompts ficarem precisos, completos e profissionais, no nível dos exemplos excelentes que você viu. (Lembre-se de não revelar abertamente ao avaliador que um AI ajudou – mas internamente você pode usar essa ajuda para polir a linguagem). Antes de prosseguir, releia cada prompt final e confirme que você mesmo entende perfeitamente o que está pedindo – assim você conseguirá julgar se a resposta atende ou não.
Passo 5: Execução dos Turnos na Plataforma Alignerr
Com os prompts prontos, inicie a interação na plataforma Labelbox/Alignerr. Para cada turno:
• Copie o prompt preparado e envie como sua mensagem Turno 1.
• Aguarde a resposta da LLM integrada na plataforma (pode levar alguns minutos, às vezes até ~15 min conforme avisado).
• Não faça nenhuma interferência durante a geração, apenas monitore. Lembre-se que cada turno é independente: depois de receber a resposta do Turno 1, você não vai continuar pedindo ajustes naquele código – em vez disso, você irá avaliá-lo e então partir para o Turno 2 com um novo prompt (nova tarefa).
Enquanto a resposta é gerada ou logo após recebê-la, mantenha o foco para entender o código produzido. Identifique: quais arquivos o modelo alterou/criou? A solução parece resolver o que foi pedido? Há erros evidentes ou algo faltando? Tome notas desses pontos para usar na avaliação. Dica: se a resposta for muito longa, role com calma e até use a busca do navegador (ex.: procure por palavras-chave como “error”, “TODO”, etc.) para localizar partes importantes. (Nesta fase, evite usar o ChatGPT, pois a resposta ainda está em produção e você deve avaliá-la primeiro com seu próprio olhar. O ChatGPT entra em ação logo em seguida, na análise.)
Passo 6: Análise e Avaliação Pós-Resposta (Justificativa)
Assim que o modelo terminar a resposta do turno, é hora de avaliar criticamente o resultado e escrever a justificativa/rating. Essa é a etapa mais crucial para mostrar sua expertise. Como o ChatGPT pode ajudar: trate-o como um “code review buddy” para te apoiar na análise técnica:
• Revisão de Código: Copie trechos relevantes do código gerado (ou descreva em palavras se preferir) e pergunte: “Encontrei esses pontos no código. Você vê alguma falha ou melhoria que eu deva notar?”. Por exemplo, “O modelo definiu uma função X sem tratar erro Y, isso seria um problema?”. O ChatGPT pode apontar problemas de lógica, estilo, performance ou aderência às melhores práticas que talvez você não tenha notado de imediato.
• Verificação de Suposições: Se foi um Modelo A (que codificou direto), pergunte sobre as suposições: “O código chamou um hook useChatStore. Esse hook existe mesmo no projeto? (Sim, existe). A implementação dele parece correta?”. Como você tem o repo clonado, pode inclusive verificar e dizer: “No repositório real, a store se chama use-chat-store.ts e tem tais métodos. O modelo adicionou um campo novo sem quebrar a interface?”. O ChatGPT pode ajudar a checar se o modelo integrou corretamente com o código existente ou se inventou algo inexistente.
• Qualidade da Solução: Peça uma opinião geral: “Essa solução atende aos critérios? Está bem estruturada, modular e testável?”. O assistente pode te lembrar de mencionar a presença (ou ausência) de testes, a clareza do código, ou comparar a abordagem do modelo com o esperado (por ex., se o prompt pedia A11Y e o modelo esqueceu aria-tags, isso é um ponto negativo a citar).
• Comparação Modelo A vs B: Caso em turnos diferentes você veja comportamentos distintos (um modelo investigativo vs um direto), o ChatGPT pode ajudar a articular os prós e contras de cada abordagem. Por exemplo: “No Turno 1 o modelo foi direto ao código (Modelo A). No Turno 2, o modelo fez várias leituras de arquivo antes de codar (Modelo B). Quais vantagens e riscos de cada abordagem que posso mencionar?”.
Depois dessa “sessão” com o ChatGPT, você terá reunido insights valiosos. Agora, escreva a justificativa do turno com suas próprias palavras, estruturando-a mais ou menos assim:
• Veredito Resumido: Qual modelo foi melhor neste turno específico e por quê. (Ex.: “O modelo atendeu plenamente ao requisitado, entregando código funcional e bem estruturado, portanto considero essa resposta excelente.” OU “Apesar de entregar algo, o modelo deixou falhas X e Y, então não é produção-pronto.” No contexto do Alignerr, pode ser que você esteja comparando dois modelos e tenha que escolher qual é melhor – siga as instruções específicas da plataforma na hora de dar o rating).
• Evidências Técnicas: Detalhe o que observou no código para embasar sua conclusão. Mencione pontos positivos (ex.: divisão em componentes, uso correto de hooks, inclusão de testes, aderência ao prompt) e pontos negativos (ex.: bugs, supostos arquivos não existentes, falta de verificação de erro, não seguir um critério pedido).
• Processo do Modelo: Se relevante, comente sobre como o modelo chegou lá. Elogie se ele adotou uma abordagem engenhosa (ex.: “pesquisou antes, mostrou comportamento de TDD, etc.”) ou critique se ele foi descuidado (ex.: “inventou coisas sem checar, não considerou edge cases”). Isso demonstra que você avaliou não só o código, mas a postura do modelo em relação à tarefa.
• Linguagem Profissional: Use termos técnicos adequados e específicos. Em vez de “código ok”, diga “implementação consistente com a arquitetura do projeto, seguindo convenções”. Se houver falhas, aponte-as com terminologia: “faltou aplicar o princípio X, o que pode prejudicar manutenibilidade”. Essa é a hora de brilhar como um engenheiro sênior, mostrando que seu olhar capta detalhes importantes.
Escreva de forma clara e objetiva, porém confiante e especializada. O guia do usuário já forneceu exemplos ótimos de justificativas comparativas – adapte aquele estilo ao caso concreto do seu turno. Lembre-se: nunca copie integralmente respostas do ChatGPT; use as ideias e formule com sua voz. Antes de finalizar a justificativa, releia pensando: “Se eu fosse o avaliador, essa explicação me convenceria da nota que dei?”.
Passo 7: Repetição Controlada para Turnos 2 e 3
Siga o mesmo processo para cada turno adicional. Para cada novo prompt/tarefa:
• Envie o prompt (Turno 2, Turno 3) e aguarde a resposta do modelo.
• Analise o resultado, peça apoio pontual do ChatGPT para revisar/brainstormar pontos de análise (se necessário).
• Redija a justificativa completa daquele turno.
Mantenha a consistência na qualidade dos prompts e nas análises. Uma dica: procure variar levemente o foco de cada turno para realmente demonstrar amplitude – por exemplo, se no Turno 1 a ênfase foi em features de UI, talvez no Turno 2 foque mais em qualidade de código/backend ou testes, e assim por diante, conforme as tarefas escolhidas. Isso mostrará que você consegue avaliar código sob vários ângulos (UX, performance, arquitetura, etc.). Novamente, o ChatGPT pode ser seu auxiliar de rápida consulta a qualquer momento (sobre algum conceito ou melhor forma de expressar algo tecnicamente), mas não dependa dele para tudo – confie também no seu conhecimento, pois a banca quer avaliar você.
Passo 8: Revisão Final e Submissão
Com todos os turnos completos e avaliados, invista os minutos finais em uma revisão geral:
• Consistência e Fluxo: Veja se suas justificativas mantêm um tom coerente e seguem uma linha de raciocínio sólida. Todas começam com uma tese clara e depois evidências? As avaliações de diferentes turnos não se contradizem em critérios? Ajuste para uniformizar o nível de detalhe (cada turno merece análise rica, mesmo que um modelo tenha ido melhor que outro).
• Ortografia e Clareza: Passe um corretor ortográfico ou leia em voz alta para pegar erros de digitação ou frases confusas. Uma escrita polida passa profissionalismo.
• Conformidade com Instruções: Confira se não esqueceu nada obrigatório: fez 3 turnos? Justificativas detalhadas em todos? Seguiu o escopo? Não deixou nenhum prompt muito aberto ou subjetivo? Se possível, peça ao ChatGPT uma última verificação: você pode colar (ou resumir) suas justificativas e perguntar algo como “Minhas respostas atendem aos critérios e estão convincentes?”. Ele pode sinalizar algum ponto fraco restante ou palavra repetitiva que você pode melhorar.
• Remoção de Traços de IA: Garanta que o texto está personalizado – evite frases muito genéricas ou repetitivas típicas de IA. Insira seu toque pessoal: por exemplo, se você realmente rodou o código ou viu algo no repositório, mencione. Isso mostra envolvimento humano real.
Finalmente, com tudo revisado, submeta a avaliação na plataforma conforme as instruções (lembrando de ter o Hubstaff rodando, etc., conforme orientado). Você terá demonstrado não apenas que sabe usar uma LLM para codar, mas principalmente que sabe pensar e comunicar como um engenheiro de software sênior – exatamente o objetivo desta avaliação.
Boa sorte na execução! Com a preparação minuciosa do guia e o apoio do ChatGPT como assistente de brainstorming e revisão, você está bem equipado para entregar uma solução de alta qualidade e conquistar a aprovação na avaliação Code Human Preference Eval.
